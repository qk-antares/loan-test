import pandas as pd
import numpy as np
from sklearn.model_selection import cross_validate, train_test_split, cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import roc_auc_score, accuracy_score, recall_score, f1_score, precision_score
from imblearn.over_sampling import SMOTE
from imblearn.combine import SMOTETomek
from sklearn.utils.class_weight import compute_class_weight
import joblib
import os
from typing import Dict, List, Tuple, Any
import warnings
from ml_data_loader import DataLoader
warnings.filterwarnings('ignore')


class LoanDistributionModel:
    """
    è´·æ¬¾åˆ†å‘æ™ºèƒ½å†³ç­–æ¨¡å‹ - é‡æ„ç‰ˆæœ¬

    ä¸»è¦åŠŸèƒ½ï¼š
    1. å¤šæ ‡ç­¾åˆ†ç±»ï¼šé¢„æµ‹æ¯ä¸ªåˆä½œæ–¹çš„é€šè¿‡æ¦‚ç‡
    2. æ™ºèƒ½æ’åºï¼šåŸºäºé€šè¿‡æ¦‚ç‡è¿›è¡Œæ’åºæ¨è
    3. å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼šå¤šç§ç­–ç•¥åº”å¯¹æ­£è´Ÿæ ·æœ¬ä¸å‡è¡¡
    """

    def __init__(self, train_data_dir: str = None, test_data_dir: str = None):
        """
        åˆå§‹åŒ–æ¨¡å‹

        Args:
            train_data_dir: è®­ç»ƒæ•°æ®ç›®å½•ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨å¯»æ‰¾ä¸Šçº§ç›®å½•
            test_data_dir: æµ‹è¯•æ•°æ®ç›®å½•ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨å¯»æ‰¾ä¸Šçº§ç›®å½•
        """


        self.partners = []  # åˆä½œæ–¹åˆ—è¡¨
        self.feature_columns = []  # ç‰¹å¾åˆ—å
        self.models = {}  # æ¯ä¸ªåˆä½œæ–¹çš„æ¨¡å‹
        self.encoders = {}  # ç¼–ç å™¨
        self.scaler = StandardScaler()

        # å®šä¹‰ä½¿ç”¨çš„ç‰¹å¾åˆ—è¡¨ï¼ˆä¿æŒä¸å˜ï¼‰
        self.feature_list = [
            'amount',
            'bankCardInfo.bankCode',
            'city',
            'companyInfo.companyName',
            'companyInfo.industry',
            'companyInfo.occupation',
            'customerSource',
            'degree',
            'idInfo.birthDate',
            'idInfo.gender',
            'idInfo.nation',
            'idInfo.validityDate',
            'income',
            'jobFunctions',
            'linkmanList.0.relationship',
            'linkmanList.1.relationship',
            'maritalStatus',
            'pictureInfo.0.faceScore',
            'province',
            'purpose',
            'resideFunctions',
            'term',
            'deviceInfo.osType',
            'deviceInfo.isCrossDomain',
            'deviceInfo.applyPos'
        ]

        # å…¬å¸åç§°è¿‡æ»¤è§„åˆ™
        self.company_filter_rules = self._init_company_filters()

    def _init_company_filters(self) -> Dict[str, List[str]]:
        """
        åˆå§‹åŒ–å…¬å¸åç§°è¿‡æ»¤è§„åˆ™

        Returns:
            è¿‡æ»¤è§„åˆ™å­—å…¸
        """
        return {
            'AWJ': ['å…¬å®‰å±€', 'è­¦å¯Ÿ', 'æ³•é™¢', 'å†›é˜Ÿ', 'æ£€å¯Ÿé™¢', 'åŸå¸‚ç®¡ç†å±€', 'å¾‹å¸ˆ', 'è®°è€…', 'è´·æ¬¾', 'é‡‘è', 'æ‰§è¡Œå±€',
                    'ç›‘ç‹±', 'äº¤é€šè­¦å¯Ÿ', 'æ´¾å‡ºæ‰€', 'åˆ‘äº‹ä¾¦æŸ¥éƒ¨é—¨', 'äº¤è­¦', 'åˆ‘ä¾¦'],
            'RONG': ['å­¦æ ¡', 'å°å­¦', 'ä¸­å­¦', 'å¤§å­¦', 'å­¦é™¢', 'å…¬æ£€æ³•'],
        }


    def preprocess_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ç‰¹å¾é¢„å¤„ç†

        Args:
            df: åŸå§‹æ•°æ®

        Returns:
            å¤„ç†åçš„ç‰¹å¾æ•°æ®
        """
        print("å¼€å§‹ç‰¹å¾é¢„å¤„ç†...")
        processed_df = df.copy()

        # 1. åªä¿ç•™é€‰æ‹©çš„ç‰¹å¾åˆ—
        available_features = [col for col in self.feature_list if col in processed_df.columns]
        missing_features = [col for col in self.feature_list if col not in processed_df.columns]

        if missing_features:
            print(f"è­¦å‘Š: ä»¥ä¸‹ç‰¹å¾åœ¨æ•°æ®ä¸­ç¼ºå¤±: {missing_features}")

        # æ·»åŠ å¿…è¦çš„æ ‡è¯†åˆ—
        required_cols = ['partner', 'partner_label']
        columns_to_keep = available_features + required_cols

        processed_df = processed_df[columns_to_keep]
        print(f"ä½¿ç”¨ {len(available_features)} ä¸ªç‰¹å¾")

        # 2. å…¬å¸åç§°è¿‡æ»¤è§„åˆ™ç‰¹å¾
        if 'companyInfo.companyName' in processed_df.columns:
            company_series = processed_df['companyInfo.companyName'].astype(str).fillna("")

            # éå†æ‰€æœ‰ç±»åˆ«ï¼ˆå¦‚ AWJã€RONGï¼‰
            for rule_name, keywords in self.company_filter_rules.items():
                for keyword in keywords:
                    # åˆ—åæ ¼å¼ç¤ºä¾‹ï¼š company_AWJ_å…¬å®‰å±€
                    col_name = f"company_{rule_name}_{keyword}"
                    processed_df[col_name] = company_series.apply(
                        lambda x: 1 if keyword in x else 0
                    )

        # 3. å­¦å†ç¼–ç  (JUNIOR=1, ..., DOCTOR=6)
        if 'degree' in processed_df.columns:
            degree_mapping = {
                'JUNIOR': 1, 'SENIOR': 2, 'COLLEGE': 3,
                'BACHELOR': 4, 'MASTER': 5, 'DOCTOR': 6
            }
            # ç¡®ä¿åŸå§‹NaNåœ¨æ˜ å°„å‰è¢«å¤„ç†ï¼Œæˆ–æ˜ å°„åå†æ¬¡å¤„ç†
            processed_df['degree_encoded'] = processed_df['degree'].map(degree_mapping)
            processed_df['degree_encoded'] = processed_df['degree_encoded'].fillna(0)  # å¡«å……ä¸º0ï¼Œè¡¨ç¤ºæœªçŸ¥æˆ–æœ€ä½å­¦å†
            processed_df = processed_df.drop('degree', axis=1)

        # 4. æ”¶å…¥ç­‰çº§ç¼–ç  (A=1, B=2, C=3, D=4)
        if 'income' in processed_df.columns:
            income_mapping = {'A': 1, 'B': 2, 'C': 3, 'D': 4}
            processed_df['income_encoded'] = processed_df['income'].map(income_mapping)
            processed_df['income_encoded'] = processed_df['income_encoded'].fillna(0)  # å¡«å……ä¸º0ï¼Œè¡¨ç¤ºæœªçŸ¥æˆ–æœ€ä½æ”¶å…¥
            processed_df = processed_df.drop('income', axis=1)

        # 5. å¤„ç†åˆ†ç±»ç‰¹å¾
        categorical_features = [
            'bankCardInfo.bankCode', 'city', 'companyInfo.industry',
            'companyInfo.occupation', 'customerSource', 'idInfo.gender',
            'idInfo.nation', 'jobFunctions', 'linkmanList.0.relationship',
            'linkmanList.1.relationship', 'maritalStatus', 'province',
            'purpose', 'resideFunctions', 'deviceInfo.osType',
            'deviceInfo.isCrossDomain', 'deviceInfo.applyPos'
        ]

        # åªå¤„ç†å®é™…å­˜åœ¨çš„åˆ†ç±»ç‰¹å¾
        categorical_features = [f for f in categorical_features if f in processed_df.columns]

        for feature in categorical_features:
            if feature in processed_df.columns:
                # 1. å¤„ç†ç¼ºå¤±å€¼ (ç”¨å­—ç¬¦ä¸²'UNKNOWN')
                processed_df[feature] = processed_df[feature].fillna('UNKNOWN')

                # 2. ç¡®ä¿æ•´ä¸ªåˆ—éƒ½æ˜¯å­—ç¬¦ä¸²ç±»å‹
                processed_df[feature] = processed_df[feature].astype(str)

                # æ ‡ç­¾ç¼–ç 
                if feature not in self.encoders:
                    self.encoders[feature] = LabelEncoder()

                    # è·å–æ‰€æœ‰å”¯ä¸€å€¼ï¼Œå¹¶ç¡®ä¿ 'UNKNOWN' åŒ…å«åœ¨å†…ï¼Œä»¥ä¾¿ç¼–ç å™¨èƒ½å¤Ÿè¯†åˆ«å®ƒ
                    unique_values_for_fit = list(processed_df[feature].unique())
                    if 'UNKNOWN' not in unique_values_for_fit:
                        unique_values_for_fit.append('UNKNOWN')

                    self.encoders[feature].fit(unique_values_for_fit)
                else:
                    # é¢„æµ‹é˜¶æ®µï¼šå¤„ç†æµ‹è¯•é›†ä¸­çš„æœªè§ç±»åˆ«
                    unseen_labels_set = set(processed_df[feature].unique()) - set(self.encoders[feature].classes_)

                    if unseen_labels_set:
                        total_unseen = len(unseen_labels_set)
                        unseen_labels_list = sorted(list(unseen_labels_set))
                        display_labels = unseen_labels_list[:10]

                        print(
                            f"  è­¦å‘Š: ç‰¹å¾ '{feature}' åœ¨æµ‹è¯•é›†ä¸­å‘ç° {total_unseen} ä¸ªæœªè§ç±»åˆ«ï¼Œå‰10ä¸ªä¸º: {display_labels}ï¼Œå°†æ›¿æ¢ä¸º 'UNKNOWN'")
                        processed_df[feature] = processed_df[feature].replace(list(unseen_labels_set), 'UNKNOWN')

                # è½¬æ¢åˆ—
                processed_df[f'{feature}_encoded'] = self.encoders[feature].transform(
                    processed_df[feature]
                )

                # ç§»é™¤åŸå§‹åˆ—
                processed_df = processed_df.drop(feature, axis=1)

        # 6. å¤„ç†æ•°å€¼ç‰¹å¾çš„ç¼ºå¤±å€¼
        numerical_features = ['amount', 'idInfo.birthDate', 'idInfo.validityDate',
                              'pictureInfo.0.faceScore', 'term']
        numerical_features = [f for f in numerical_features if f in processed_df.columns]

        for feature in numerical_features:
            if feature in processed_df.columns:
                processed_df[feature] = pd.to_numeric(processed_df[feature], errors='coerce')
                # å†æ¬¡ä½¿ç”¨ä¸­ä½æ•°å¡«å……ï¼Œç¡®ä¿åœ¨æ‰€æœ‰è½¬æ¢åä¾ç„¶ä¿æŒ
                processed_df[feature] = processed_df[feature].fillna(processed_df[feature].median())

        # 7. ç§»é™¤å…¬å¸åç§°åŸå§‹åˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'companyInfo.companyName' in processed_df.columns:
            processed_df = processed_df.drop('companyInfo.companyName', axis=1)

        # === å…³é”®ä¿®æ”¹éƒ¨åˆ†ï¼šæ›´é²æ£’çš„æœ€ç»ˆNaNå¤„ç† ===
        # ç¡®ä¿æ‰€æœ‰æ•°å€¼å‹åˆ—æ²¡æœ‰NaNï¼Œä½¿ç”¨ä¸­ä½æ•°å¡«å……
        for col in processed_df.select_dtypes(include=np.number).columns:
            if processed_df[col].isnull().any():
                # ä½¿ç”¨è¯¥åˆ—çš„ä¸­ä½æ•°å¡«å……ï¼Œè€Œä¸æ˜¯æ•´ä¸ªDataFrameçš„ä¸­ä½æ•°
                median_val = processed_df[col].median()
                if pd.isna(median_val):  # å¦‚æœæ‰€æœ‰å€¼éƒ½æ˜¯NaNï¼Œä¸­ä½æ•°ä¹Ÿä¼šæ˜¯NaNï¼Œæ­¤æ—¶å¡«å……0
                    processed_df[col] = processed_df[col].fillna(0)
                else:
                    processed_df[col] = processed_df[col].fillna(median_val)

        # ç¡®ä¿æ‰€æœ‰éæ•°å€¼å‹åˆ—ï¼ˆå¦‚'partner'åˆ—ï¼Œè™½ç„¶ä¸æ˜¯ç‰¹å¾ï¼Œä½†ä¹Ÿè¦å¹²å‡€ï¼‰æ²¡æœ‰NaN
        for col in processed_df.select_dtypes(exclude=np.number).columns:
            if processed_df[col].isnull().any():
                processed_df[col] = processed_df[col].fillna('UNKNOWN_CATEGORY')  # ç”¨ä¸€ä¸ªç‰¹æ®Šçš„å­—ç¬¦ä¸²å¡«å……

        print(f"é¢„å¤„ç†å®Œæˆï¼Œç‰¹å¾ç»´åº¦: {processed_df.shape[1]}")
        return processed_df

    def _check_company_keywords(self, company_name: str, keywords: List[str]) -> int:
        """
        æ£€æŸ¥å…¬å¸åç§°æ˜¯å¦åŒ…å«å…³é”®è¯

        Args:
            company_name: å…¬å¸åç§°
            keywords: å…³é”®è¯åˆ—è¡¨

        Returns:
            1 if åŒ…å«å…³é”®è¯, 0 otherwise
        """
        if not company_name or pd.isna(company_name):
            return 0

        company_name_str = str(company_name)
        for keyword in keywords:
            if keyword in company_name_str:
                return 1
        return 0

    def prepare_training_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, Dict[str, Dict]]:
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®

        Args:
            df: é¢„å¤„ç†åçš„æ•°æ®

        Returns:
            ç‰¹å¾çŸ©é˜µXå’Œæ¯ä¸ªåˆä½œæ–¹çš„æ ‡ç­¾å­—å…¸
        """
        # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
        feature_columns = [col for col in df.columns
                           if col not in ['partner', 'partner_label']]

        self.feature_columns = feature_columns
        X = df[feature_columns].values

        # ç‰¹å¾æ ‡å‡†åŒ–
        X = self.scaler.fit_transform(X)

        # ä¸ºæ¯ä¸ªåˆä½œæ–¹å‡†å¤‡æ ‡ç­¾
        Y_dict = {}
        for partner in self.partners:
            partner_mask = df['partner'] == partner
            partner_indices = df.index[partner_mask].tolist()

            if len(partner_indices) > 0:
                Y_dict[partner] = {
                    'X_indices': partner_indices,
                    'labels': df.loc[partner_mask, 'partner_label'].values
                }

        print(f"è®­ç»ƒæ•°æ®å‡†å¤‡å®Œæˆ:")
        print(f"  ç‰¹å¾ç»´åº¦: {X.shape}")
        print(f"  å„åˆä½œæ–¹æ•°æ®åˆ†å¸ƒ:")
        for partner in self.partners:
            if partner in Y_dict:
                partner_data = Y_dict[partner]
                pass_rate = partner_data['labels'].mean()
                print(f"    {partner}: {len(partner_data['labels'])} æ ·æœ¬, é€šè¿‡ç‡ {pass_rate:.3f}")
            else:
                print(f"    {partner}: æ— æ•°æ®")

        return X, Y_dict

    def train_models_with_imbalance_handling(self, X: np.ndarray, Y_dict: Dict[str, Dict],
                                             strategy: str = "class_weight") -> Dict[str, Dict]:
        """
        è®­ç»ƒæ¨¡å‹ - å¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜
        """
        print(f"å¼€å§‹è®­ç»ƒæ¨¡å‹ - ä½¿ç”¨ {strategy} ç­–ç•¥å¤„ç†ç±»åˆ«ä¸å¹³è¡¡...")

        strategies_results = {}
        trained_models = {}

        # ä¸ºæ¯ä¸ªåˆä½œæ–¹è®­ç»ƒå•ç‹¬çš„åˆ†ç±»å™¨
        for partner in self.partners:
            if partner not in Y_dict:
                print(f"\n{partner}: æ— è®­ç»ƒæ•°æ®ï¼Œè·³è¿‡")
                continue

            print(f"\nè®­ç»ƒ {partner} æ¨¡å‹...")

            partner_data = Y_dict[partner]
            X_partner = X[partner_data['X_indices']]
            y_partner = partner_data['labels']

            # æ£€æŸ¥æ•°æ®é‡å’Œç±»åˆ«åˆ†å¸ƒ
            if len(y_partner) < 10:
                print(f"  æ•°æ®é‡å¤ªå°‘ ({len(y_partner)} æ ·æœ¬), è·³è¿‡è®­ç»ƒ")
                continue

            pos_count = np.sum(y_partner)
            neg_count = len(y_partner) - pos_count
            pos_rate = pos_count / len(y_partner)

            print(f"  æ•°æ®åˆ†å¸ƒ: æ­£ç±» {pos_count}, è´Ÿç±» {neg_count}, æ­£ç±»ç‡ {pos_rate:.3f}")

            if pos_count < 2:
                print(f"  æ­£ç±»æ ·æœ¬å¤ªå°‘ ({pos_count} ä¸ª), è·³è¿‡è®­ç»ƒ")
                continue

            # æ ¹æ®ç­–ç•¥é€‰æ‹©ä¸åŒçš„å¤„ç†æ–¹æ³•
            if strategy == "class_weight":
                model, cv_results = self._train_with_class_weight(X_partner, y_partner, partner)
            elif strategy == "smote":
                model, cv_results = self._train_with_smote(X_partner, y_partner, partner)
            elif strategy == "combine":
                model, cv_results = self._train_with_combine_sampling(X_partner, y_partner, partner)
            elif strategy == "threshold":
                model, cv_results = self._train_with_threshold_tuning(X_partner, y_partner, partner)
            else:  # æ•è· "baseline" æˆ–å…¶ä»–æœªå®šä¹‰çš„ç­–ç•¥
                model, cv_results = self._train_baseline(X_partner, y_partner, partner)

            if model is not None:
                # ğŸ”§ å…³é”®ä¿®æ”¹ï¼šæ€»æ˜¯ä¿å­˜å½“å‰ç­–ç•¥çš„ç»“æœï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
                strategies_results[partner] = cv_results

                # ä½†åªä¿å­˜AUCæ›´é«˜çš„æ¨¡å‹
                current_auc = cv_results['test_roc_auc'].mean()

                if partner in self.models:
                    previous_auc = getattr(self.models[partner], 'best_auc', 0)
                    if current_auc > previous_auc:
                        trained_models[partner] = model
                        setattr(trained_models[partner], 'best_auc', current_auc)
                        setattr(trained_models[partner], 'best_strategy', strategy)
                else:
                    trained_models[partner] = model
                    setattr(trained_models[partner], 'best_auc', current_auc)
                    setattr(trained_models[partner], 'best_strategy', strategy)

        # æ›´æ–°æ¨¡å‹å­—å…¸ï¼ˆåªæ›´æ–°æœ‰æ”¹è¿›çš„æ¨¡å‹ï¼‰
        self.models.update(trained_models)

        return strategies_results

    def _train_baseline(self, X: np.ndarray, y: np.ndarray, partner: str):
        """
        åŸºçº¿æ¨¡å‹è®­ç»ƒï¼Œä¸è¿›è¡Œç‰¹æ®Šä¸å¹³è¡¡å¤„ç†ã€‚
        """
        print(f"  ç­–ç•¥: åŸºçº¿æ¨¡å‹ (æ— ç‰¹æ®Šä¸å¹³è¡¡å¤„ç†)")

        model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            min_samples_split=min(5, len(y) // 10), # ä¿æŒä¸å…¶ä»–æ¨¡å‹ä¸€è‡´çš„å‚æ•°
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )

        # äº¤å‰éªŒè¯
        cv_results = self._evaluate_model(model, X, y)
        self._print_results(partner, cv_results)

        # åœ¨å…¨é‡æ•°æ®ä¸Šè®­ç»ƒæœ€ç»ˆæ¨¡å‹
        model.fit(X, y)

        return model, cv_results

    def _train_with_class_weight(self, X: np.ndarray, y: np.ndarray, partner: str):
        """ä½¿ç”¨ç±»åˆ«æƒé‡å¤„ç†ä¸å¹³è¡¡"""
        print(f"  ç­–ç•¥: ç±»åˆ«æƒé‡å¹³è¡¡")

        # è®¡ç®—ç±»åˆ«æƒé‡
        try:
            class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)
            class_weight_dict = {0: class_weights[0], 1: class_weights[1]}
        except:
            class_weight_dict = 'balanced'

        print(f"  ç±»åˆ«æƒé‡: {class_weight_dict}")

        model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            min_samples_split=min(5, len(y) // 10),
            min_samples_leaf=2,
            class_weight=class_weight_dict,
            random_state=42,
            n_jobs=-1
        )

        # äº¤å‰éªŒè¯
        cv_results = self._evaluate_model(model, X, y)
        self._print_results(partner, cv_results)

        # åœ¨å…¨é‡æ•°æ®ä¸Šè®­ç»ƒæœ€ç»ˆæ¨¡å‹
        model.fit(X, y)

        return model, cv_results

    def _train_with_smote(self, X: np.ndarray, y: np.ndarray, partner: str):
        """ä½¿ç”¨SMOTEè¿‡é‡‡æ ·å¤„ç†ä¸å¹³è¡¡"""
        print(f"  ç­–ç•¥: SMOTEè¿‡é‡‡æ ·")

        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å°‘æ•°ç±»æ ·æœ¬è¿›è¡ŒSMOTE
            min_samples = min(np.bincount(y.astype(int)))
            if min_samples < 2:
                print(f"  å°‘æ•°ç±»æ ·æœ¬å¤ªå°‘ï¼Œå›é€€åˆ°ç±»åˆ«æƒé‡æ–¹æ³•")
                return self._train_with_class_weight(X, y, partner)

            # ä½¿ç”¨SMOTEè¿›è¡Œè¿‡é‡‡æ ·
            smote = SMOTE(random_state=42, k_neighbors=min(5, min_samples - 1))

            # åˆ†å±‚äº¤å‰éªŒè¯
            skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
            cv_scores = {'auc': [], 'accuracy': [], 'recall': [], 'f1': [], 'precision': []}

            for train_idx, val_idx in skf.split(X, y):
                X_train_fold, X_val_fold = X[train_idx], X[val_idx]
                y_train_fold, y_val_fold = y[train_idx], y[val_idx]

                # åœ¨è®­ç»ƒé›†ä¸Šåº”ç”¨SMOTE
                X_train_resampled, y_train_resampled = smote.fit_resample(X_train_fold, y_train_fold)

                # è®­ç»ƒæ¨¡å‹
                model_fold = RandomForestClassifier(
                    n_estimators=100,
                    max_depth=10,
                    min_samples_split=5,
                    min_samples_leaf=2,
                    random_state=42,
                    n_jobs=-1
                )
                model_fold.fit(X_train_resampled, y_train_resampled)

                # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
                y_pred_proba = model_fold.predict_proba(X_val_fold)[:, 1]
                y_pred = (y_pred_proba > 0.5).astype(int)

                cv_scores['auc'].append(roc_auc_score(y_val_fold, y_pred_proba))
                cv_scores['accuracy'].append(accuracy_score(y_val_fold, y_pred))
                cv_scores['recall'].append(recall_score(y_val_fold, y_pred, zero_division=0))
                cv_scores['f1'].append(f1_score(y_val_fold, y_pred, zero_division=0))
                cv_scores['precision'].append(precision_score(y_val_fold, y_pred, zero_division=0))

            # è®¡ç®—å¹³å‡æ€§èƒ½
            cv_results = {f'test_{k}': np.array(v) for k, v in cv_scores.items()}
            cv_results['test_roc_auc'] = cv_results.pop('test_auc')

            self._print_results(partner, cv_results)

            # åœ¨å…¨é‡æ•°æ®ä¸Šåº”ç”¨SMOTEå¹¶è®­ç»ƒæœ€ç»ˆæ¨¡å‹
            X_resampled, y_resampled = smote.fit_resample(X, y)
            print(f"  SMOTEå: åŸå§‹ {len(y)} -> å¹³è¡¡ {len(y_resampled)} æ ·æœ¬")

            model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                n_jobs=-1
            )
            model.fit(X_resampled, y_resampled)

            return model, cv_results

        except Exception as e:
            print(f"  SMOTEå¤±è´¥: {e}, å›é€€åˆ°ç±»åˆ«æƒé‡æ–¹æ³•")
            return self._train_with_class_weight(X, y, partner)

    def _train_with_combine_sampling(self, X: np.ndarray, y: np.ndarray, partner: str):
        """ä½¿ç”¨SMOTE+Tomekç»„åˆé‡‡æ ·å¤„ç†ä¸å¹³è¡¡"""
        print(f"  ç­–ç•¥: SMOTE+Tomekç»„åˆé‡‡æ ·")

        try:
            min_samples = min(np.bincount(y.astype(int)))
            if min_samples < 2:
                return self._train_with_class_weight(X, y, partner)

            # ä½¿ç”¨SMOTETomekç»„åˆæ–¹æ³•
            smote_tomek = SMOTETomek(random_state=42)

            # äº¤å‰éªŒè¯
            skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
            cv_scores = {'auc': [], 'accuracy': [], 'recall': [], 'f1': [], 'precision': []}

            for train_idx, val_idx in skf.split(X, y):
                X_train_fold, X_val_fold = X[train_idx], X[val_idx]
                y_train_fold, y_val_fold = y[train_idx], y[val_idx]

                X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train_fold, y_train_fold)

                model_fold = RandomForestClassifier(
                    n_estimators=100,
                    max_depth=10,
                    min_samples_split=5,
                    min_samples_leaf=2,
                    random_state=42,
                    n_jobs=-1
                )
                model_fold.fit(X_train_resampled, y_train_resampled)

                y_pred_proba = model_fold.predict_proba(X_val_fold)[:, 1]
                y_pred = (y_pred_proba > 0.5).astype(int)

                cv_scores['auc'].append(roc_auc_score(y_val_fold, y_pred_proba))
                cv_scores['accuracy'].append(accuracy_score(y_val_fold, y_pred))
                cv_scores['recall'].append(recall_score(y_val_fold, y_pred, zero_division=0))
                cv_scores['f1'].append(f1_score(y_val_fold, y_pred, zero_division=0))
                cv_scores['precision'].append(precision_score(y_val_fold, y_pred, zero_division=0))

            cv_results = {f'test_{k}': np.array(v) for k, v in cv_scores.items()}
            cv_results['test_roc_auc'] = cv_results.pop('test_auc')

            self._print_results(partner, cv_results)

            # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
            X_resampled, y_resampled = smote_tomek.fit_resample(X, y)
            print(f"  ç»„åˆé‡‡æ ·å: åŸå§‹ {len(y)} -> å¤„ç†å {len(y_resampled)} æ ·æœ¬")

            model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                n_jobs=-1
            )
            model.fit(X_resampled, y_resampled)

            return model, cv_results

        except Exception as e:
            print(f"  ç»„åˆé‡‡æ ·å¤±è´¥: {e}, å›é€€åˆ°ç±»åˆ«æƒé‡æ–¹æ³•")
            return self._train_with_class_weight(X, y, partner)

    def _train_with_threshold_tuning(self, X: np.ndarray, y: np.ndarray, partner: str):
        """é€šè¿‡è°ƒæ•´åˆ†ç±»é˜ˆå€¼å¤„ç†ä¸å¹³è¡¡"""
        print(f"  ç­–ç•¥: åˆ†ç±»é˜ˆå€¼è°ƒä¼˜")

        model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            min_samples_split=min(5, len(y) // 10),
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )

        # äº¤å‰éªŒè¯å¯»æ‰¾æœ€ä¼˜é˜ˆå€¼
        skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
        best_threshold = 0.5
        best_f1 = 0

        cv_scores = {'auc': [], 'accuracy': [], 'recall': [], 'f1': [], 'precision': []}

        for train_idx, val_idx in skf.split(X, y):
            X_train_fold, X_val_fold = X[train_idx], X[val_idx]
            y_train_fold, y_val_fold = y[train_idx], y[val_idx]

            model_fold = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                n_jobs=-1
            )
            model_fold.fit(X_train_fold, y_train_fold)

            # è·å–é¢„æµ‹æ¦‚ç‡
            y_pred_proba = model_fold.predict_proba(X_val_fold)[:, 1]

            # å¯»æ‰¾æœ€ä¼˜é˜ˆå€¼
            thresholds = np.arange(0.1, 0.9, 0.05)
            fold_best_f1 = 0
            fold_best_threshold = 0.5

            for threshold in thresholds:
                y_pred_thresh = (y_pred_proba >= threshold).astype(int)
                f1 = f1_score(y_val_fold, y_pred_thresh, zero_division=0)
                if f1 > fold_best_f1:
                    fold_best_f1 = f1
                    fold_best_threshold = threshold

            # ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼è¿›è¡Œé¢„æµ‹
            y_pred = (y_pred_proba >= fold_best_threshold).astype(int)

            cv_scores['auc'].append(roc_auc_score(y_val_fold, y_pred_proba))
            cv_scores['accuracy'].append(accuracy_score(y_val_fold, y_pred))
            cv_scores['recall'].append(recall_score(y_val_fold, y_pred, zero_division=0))
            cv_scores['f1'].append(f1_score(y_val_fold, y_pred, zero_division=0))
            cv_scores['precision'].append(precision_score(y_val_fold, y_pred, zero_division=0))

            if fold_best_f1 > best_f1:
                best_f1 = fold_best_f1
                best_threshold = fold_best_threshold

        print(f"  æœ€ä¼˜é˜ˆå€¼: {best_threshold:.3f}")

        cv_results = {f'test_{k}': np.array(v) for k, v in cv_scores.items()}
        cv_results['test_roc_auc'] = cv_results.pop('test_auc')

        self._print_results(partner, cv_results)

        # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
        model.fit(X, y)

        # ä¿å­˜æœ€ä¼˜é˜ˆå€¼ä¾›é¢„æµ‹ä½¿ç”¨
        setattr(model, 'optimal_threshold', best_threshold)

        return model, cv_results

    def _evaluate_model(self, model, X: np.ndarray, y: np.ndarray):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        scoring = {
            'roc_auc': 'roc_auc',
            'accuracy': 'accuracy',
            'recall': 'recall',
            'f1': 'f1',
            'precision': 'precision',
        }

        cv_results = cross_validate(
            model, X, y, scoring=scoring,
            cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),
            error_score='raise'
        )

        return cv_results

    def _print_results(self, partner: str, cv_results: dict):
        """æ‰“å°ç»“æœ"""
        print(f"  {partner} äº¤å‰éªŒè¯ç»“æœ:")
        print(f"    AUC: {cv_results['test_roc_auc'].mean():.3f} (Â±{cv_results['test_roc_auc'].std():.3f})")
        print(f"    å‡†ç¡®ç‡: {cv_results['test_accuracy'].mean():.3f} (Â±{cv_results['test_accuracy'].std():.3f})")
        print(f"    æŸ¥å‡†ç‡: {cv_results['test_precision'].mean():.3f} (Â±{cv_results['test_precision'].std():.3f})")
        print(f"    å¬å›ç‡: {cv_results['test_recall'].mean():.3f} (Â±{cv_results['test_recall'].std():.3f})")
        print(f"    F1åˆ†æ•°: {cv_results['test_f1'].mean():.3f} (Â±{cv_results['test_f1'].std():.3f})")

    def compare_imbalance_strategies(self, X: np.ndarray, Y_dict: Dict[str, Dict]):
        """
        æ¯”è¾ƒä¸åŒçš„ä¸å¹³è¡¡å¤„ç†ç­–ç•¥
        """
        print("=== æ¯”è¾ƒä¸åŒçš„ç±»åˆ«ä¸å¹³è¡¡å¤„ç†ç­–ç•¥ ===\n")

        # ä¿®æ­£ï¼šåœ¨ç­–ç•¥åˆ—è¡¨ä¸­æ·»åŠ  "baseline"
        strategies = ["baseline", "class_weight", "smote", "combine", "threshold"]

        all_results = {}

        for strategy in strategies:
            print(f"\n{'=' * 50}")
            print(f"ç­–ç•¥: {strategy.upper()}")
            print(f"{'=' * 50}")

            results = self.train_models_with_imbalance_handling(X, Y_dict, strategy)
            all_results[strategy] = results

        # æ€»ç»“æ¯”è¾ƒç»“æœ
        self._summarize_strategy_comparison(all_results)

        return all_results

    def _summarize_strategy_comparison(self, all_results: Dict[str, Dict]):
        """æ€»ç»“ç­–ç•¥æ¯”è¾ƒç»“æœ"""
        print(f"\n{'=' * 80}")
        print("ç­–ç•¥æ¯”è¾ƒæ€»ç»“")
        print(f"{'=' * 80}")

        for partner in self.partners:
            partner_results = {}

            for strategy, results in all_results.items():
                if partner in results:
                    cv_result = results[partner]

                    avg_f1 = cv_result['test_f1'].mean()
                    std_f1 = cv_result['test_f1'].std()
                    avg_auc = cv_result['test_roc_auc'].mean()
                    std_auc = cv_result['test_roc_auc'].std()
                    avg_recall = cv_result['test_recall'].mean()
                    std_recall = cv_result['test_recall'].std()
                    avg_precision = cv_result['test_precision'].mean()
                    std_precision = cv_result['test_precision'].std()
                    avg_accuracy = cv_result['test_accuracy'].mean()
                    std_accuracy = cv_result['test_accuracy'].std()

                    partner_results[strategy] = {
                        'auc': avg_auc, 'auc_std': std_auc,
                        'f1': avg_f1, 'f1_std': std_f1,
                        'recall': avg_recall, 'recall_std': std_recall,
                        'precision': avg_precision, 'precision_std': std_precision,
                        'accuracy': avg_accuracy, 'accuracy_std': std_accuracy
                    }

            if partner_results:
                print(f"\n{partner}:")
                sorted_strategies = sorted(partner_results.items(),
                                           key=lambda x: (x[1]['auc'], x[1]['f1'], x[1]['recall']),
                                           reverse=True)

                for i, (strategy, metrics) in enumerate(sorted_strategies):
                    status = "ğŸ† æœ€ä½³" if i == 0 else f"  #{i + 1}"
                    print(f"  {status} {strategy:15} "
                          f"AUC: {metrics['auc']:.3f} (Â±{metrics['auc_std']:.3f})  "
                          f"F1: {metrics['f1']:.3f} (Â±{metrics['f1_std']:.3f})  "
                          f"æŸ¥å‡†ç‡: {metrics['precision']:.3f} (Â±{metrics['precision_std']:.3f})  "
                          f"å¬å›ç‡: {metrics['recall']:.3f} (Â±{metrics['recall_std']:.3f})  "
                          f"å‡†ç¡®ç‡: {metrics['accuracy']:.3f} (Â±{metrics['accuracy_std']:.3f})")

    def predict_partner_probabilities(self, X: np.ndarray) -> Dict[str, np.ndarray]:
        """
        é¢„æµ‹æ¯ä¸ªåˆä½œæ–¹çš„é€šè¿‡æ¦‚ç‡
        """
        probabilities = {}

        for partner in self.partners:
            if partner in self.models:
                proba = self.models[partner].predict_proba(X)[:, 1]
                probabilities[partner] = proba

        return probabilities

    def recommend_partners(self, user_features: np.ndarray, k: int = 3,
                           min_probability: float = 0.3) -> List[Tuple[str, float]]:
        """
        ä¸ºç”¨æˆ·æ¨èåˆä½œæ–¹
        """
        probabilities = self.predict_partner_probabilities(user_features)

        filtered_partners = [
            (partner, prob[0]) for partner, prob in probabilities.items()
            if prob[0] >= min_probability
        ]

        filtered_partners.sort(key=lambda x: x[1], reverse=True)
        return filtered_partners[:k]

    def evaluate_on_test(self, test_data: pd.DataFrame):
        """åœ¨æµ‹è¯•æ•°æ®ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        print("=== åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ ===")

        # å‡†å¤‡æµ‹è¯•æ•°æ®
        feature_columns = [col for col in test_data.columns
                           if col not in ['partner', 'partner_label']]
        X_test = test_data[feature_columns].values
        X_test = self.scaler.transform(X_test)

        results = {}

        for partner in self.partners:
            if partner not in self.models:
                continue

            partner_mask = test_data['partner'] == partner
            if partner_mask.sum() == 0:
                continue

            X_partner = X_test[partner_mask]
            y_true = test_data.loc[partner_mask, 'partner_label'].values

            model = self.models[partner]
            y_pred_proba = model.predict_proba(X_partner)[:, 1]

            # ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if hasattr(model, 'optimal_threshold'):
                threshold = model.optimal_threshold
            else:
                threshold = 0.5

            y_pred = (y_pred_proba >= threshold).astype(int)

            results[partner] = {
                'auc': roc_auc_score(y_true, y_pred_proba),
                'accuracy': accuracy_score(y_true, y_pred),
                'recall': recall_score(y_true, y_pred, zero_division=0),
                'precision': precision_score(y_true, y_pred, zero_division=0),
                'f1': f1_score(y_true, y_pred, zero_division=0),
                'samples': len(y_true)
            }

        # æ‰“å°ç»“æœ
        print("\næµ‹è¯•é›†æ€§èƒ½:")
        for partner, metrics in results.items():
            print(f"{partner}: AUC={metrics['auc']:.3f}, F1={metrics['f1']:.3f}, "
                  f"æ ·æœ¬æ•°={metrics['samples']}")

        return results

    def save_model(self, model_path: str = "loan_distribution_model"):
        """ä¿å­˜æ¨¡å‹"""
        os.makedirs(model_path, exist_ok=True)

        joblib.dump(self.models, os.path.join(model_path, 'models.pkl'))
        joblib.dump(self.encoders, os.path.join(model_path, 'encoders.pkl'))
        joblib.dump(self.scaler, os.path.join(model_path, 'scaler.pkl'))
        joblib.dump(self.partners, os.path.join(model_path, 'partners.pkl'))
        joblib.dump(self.feature_columns, os.path.join(model_path, 'feature_columns.pkl'))

        print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")

    def load_model(self, model_path: str = "loan_distribution_model"):
        """åŠ è½½æ¨¡å‹"""
        self.models = joblib.load(os.path.join(model_path, 'models.pkl'))
        self.encoders = joblib.load(os.path.join(model_path, 'encoders.pkl'))
        self.scaler = joblib.load(os.path.join(model_path, 'scaler.pkl'))
        self.partners = joblib.load(os.path.join(model_path, 'partners.pkl'))
        self.feature_columns = joblib.load(os.path.join(model_path, 'feature_columns.pkl'))

        print(f"æ¨¡å‹å·²ä» {model_path} åŠ è½½")


def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºå®Œæ•´çš„è®­ç»ƒå’Œè¯„ä¼°æµç¨‹"""
    print("=== è´·æ¬¾åˆ†å‘æ™ºèƒ½å†³ç­–æ¨¡å‹ - è‡ªåŠ¨æ•°æ®åŠ è½½ç‰ˆæœ¬ ===\n")

    try:
        # 1ï¸âƒ£ åˆå§‹åŒ– DataLoaderï¼ˆæŒ‡å®š processed æ•°æ®ç›®å½•ï¼‰
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        processed_path = os.path.join(project_root, "processed")
        loader = DataLoader(processed_root=processed_path)

        # 2ï¸âƒ£ è‡ªåŠ¨åˆ’åˆ†è®­ç»ƒ / æµ‹è¯•æ—¥æœŸ
        train_start, train_end, test_start, test_end = loader.get_train_test_dates(scheme=1)

        # 3ï¸âƒ£ åŠ è½½è®­ç»ƒé›†æ•°æ®
        print("\n=== åŠ è½½è®­ç»ƒæ•°æ® ===")
        train_data = loader.load_data_range(train_start, train_end)

        # 4ï¸âƒ£ åŠ è½½æµ‹è¯•é›†æ•°æ®
        print("\n=== åŠ è½½æµ‹è¯•æ•°æ® ===")
        test_data = loader.load_data_range(test_start, test_end)

        print("\n=== æµ‹è¯•æ•°æ®åŸºæœ¬ä¿¡æ¯ ===")
        print(f"æµ‹è¯•æ•°æ®å½¢çŠ¶: {test_data.shape}")
        print(f"æµ‹è¯•æ•°æ®åˆ—å: {test_data.columns.tolist()}")
        print(f"æµ‹è¯•æ•°æ®å‰5è¡Œ:")
        print(test_data.head())

        # 5ï¸âƒ£ åˆå§‹åŒ–æ¨¡å‹
        model = LoanDistributionModel()
        model.partners = train_data["partner"].dropna().unique().tolist()

        # 6ï¸âƒ£ æ•°æ®é¢„å¤„ç†
        processed_train_data = model.preprocess_features(train_data)
        processed_test_data = model.preprocess_features(test_data)

        # 7ï¸âƒ£ å‡†å¤‡è®­ç»ƒç‰¹å¾
        X_train, Y_dict = model.prepare_training_data(processed_train_data)

        # 8ï¸âƒ£ æ¯”è¾ƒä¸åŒä¸å¹³è¡¡ç­–ç•¥
        comparison_results = model.compare_imbalance_strategies(X_train, Y_dict)

        # 9ï¸âƒ£ æµ‹è¯•é›†è¯„ä¼°
        print("\n=== æ¨¡å‹è¯„ä¼° ===")
        test_results = model.evaluate_on_test(processed_test_data)

        # ğŸ”Ÿ ä¿å­˜æ¨¡å‹
        # model_save_path = os.path.join(os.getcwd(), "trained_model")
        # model.save_model(model_save_path)
        # print(f"\nâœ… æ¨¡å‹è®­ç»ƒå’Œè¯„ä¼°å®Œæˆï¼Œå·²ä¿å­˜åˆ°: {model_save_path}")

        return model

    except Exception as e:
        print(f"âŒ è¿è¡Œè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return None


if __name__ == "__main__":
    main()
