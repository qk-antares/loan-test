"""
è´·æ¬¾åˆ†å‘æ™ºèƒ½å†³ç­–æ¨¡å‹è®¾è®¡

ä¸šåŠ¡åœºæ™¯ï¼š
- å¹³å°ä½œä¸ºä¸­ä»‹ï¼Œå°†ç”¨æˆ·è´·æ¬¾è¯·æ±‚åˆ†å‘åˆ°å¤šä¸ªä¸‹æ¸¸åˆä½œæ–¹
- æ¯ä¸ªåˆä½œæ–¹æœ‰è‡ªå·±çš„å®¡æ ¸è§„åˆ™ï¼ˆé»‘ç›’ï¼‰
- å¹³å°ç›®æ ‡ï¼šåœ¨æ•°é‡çº¦æŸä¸‹æœ€å¤§åŒ–é€šè¿‡ç‡å’Œæ”¶ç›Š
- æ•°æ®ï¼š11ä¸ªåˆä½œæ–¹çš„å†å²å®¡æ ¸ç»“æœ

æ¨¡å‹é€‰æ‹©ï¼šMulti-label Classification + Ranking
"""

import pandas as pd
import numpy as np
from sklearn.model_selection import cross_validate, train_test_split, cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.multioutput import MultiOutputClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder, MultiLabelBinarizer
from sklearn.metrics import classification_report, fbeta_score, hamming_loss, accuracy_score, make_scorer, precision_recall_curve
from imblearn.over_sampling import SMOTE, ADASYN
from imblearn.under_sampling import RandomUnderSampler, EditedNearestNeighbours
from imblearn.combine import SMOTETomek, SMOTEENN
from sklearn.utils.class_weight import compute_class_weight
import joblib
import os
import re
from typing import Dict, List, Tuple, Any
import warnings
warnings.filterwarnings('ignore')

class LoanDistributionModel:
    """
    è´·æ¬¾åˆ†å‘æ™ºèƒ½å†³ç­–æ¨¡å‹
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. å¤šæ ‡ç­¾åˆ†ç±»ï¼šé¢„æµ‹æ¯ä¸ªåˆä½œæ–¹çš„é€šè¿‡æ¦‚ç‡
    2. æ™ºèƒ½æ’åºï¼šåŸºäºé€šè¿‡æ¦‚ç‡å’Œå†å²æ”¶ç›Šè¿›è¡Œæ’åº
    3. çº¦æŸåˆ†å‘ï¼šåœ¨æ•°é‡çº¦æŸä¸‹é€‰æ‹©æœ€ä¼˜åˆä½œæ–¹ç»„åˆ
    """
    
    def __init__(self, processed_data_dir: str = "processed_data"):
        """
        åˆå§‹åŒ–æ¨¡å‹
        
        Args:
            processed_data_dir: å¤„ç†åæ•°æ®ç›®å½•
        """
        self.processed_data_dir = processed_data_dir
        self.partners = []  # åˆä½œæ–¹åˆ—è¡¨
        self.feature_columns = []  # ç‰¹å¾åˆ—å
        self.models = {}  # æ¯ä¸ªåˆä½œæ–¹çš„æ¨¡å‹
        self.encoders = {}  # ç¼–ç å™¨
        self.scaler = StandardScaler()
        self.company_filter_rules = self._init_company_filters()
        
    def _init_company_filters(self) -> Dict[str, List[str]]:
        """
        åˆå§‹åŒ–å…¬å¸åç§°è¿‡æ»¤è§„åˆ™
        
        Returns:
            è¿‡æ»¤è§„åˆ™å­—å…¸
        """
        return {
            'AWJ': ['å…¬å®‰å±€', 'è­¦å¯Ÿ', 'æ³•é™¢', 'å†›é˜Ÿ', 'æ£€å¯Ÿé™¢', 'åŸå¸‚ç®¡ç†å±€', 'å¾‹å¸ˆ', 'è®°è€…', 'è´·æ¬¾', 'é‡‘è', 'æ‰§è¡Œå±€', 'ç›‘ç‹±', 'äº¤é€šè­¦å¯Ÿ', 'æ´¾å‡ºæ‰€', 'åˆ‘äº‹ä¾¦æŸ¥éƒ¨é—¨', 'äº¤è­¦', 'åˆ‘ä¾¦'],
            'RONG': ['å­¦æ ¡', 'å°å­¦', 'ä¸­å­¦', 'å¤§å­¦', 'å­¦é™¢', 'å…¬æ£€æ³•'],
        }
    
    def load_and_combine_data(self) -> pd.DataFrame:
        """
        åŠ è½½å¹¶åˆå¹¶æ‰€æœ‰åˆä½œæ–¹æ•°æ®
        
        Returns:
            åˆå¹¶åçš„æ•°æ®é›†ï¼Œæ¯è¡ŒåŒ…å«ä¸€ä¸ªæ ·æœ¬åŠå…¶å¯¹åº”åˆä½œæ–¹çš„æ ‡ç­¾
        """
        print("å¼€å§‹åŠ è½½åˆä½œæ–¹æ•°æ®...")
        
        # è·å–æ‰€æœ‰åˆä½œæ–¹æ–‡ä»¶
        partner_files = [f for f in os.listdir(self.processed_data_dir) 
                        if f.endswith('.csv')]
        
        self.partners = [f.replace('.csv', '') for f in partner_files]
        print(f"å‘ç° {len(self.partners)} ä¸ªåˆä½œæ–¹: {self.partners}")
        
        # åŠ è½½æ‰€æœ‰åˆä½œæ–¹æ•°æ®å¹¶åˆå¹¶
        all_data = []
        
        for partner_file in partner_files:
            partner_name = partner_file.replace('.csv', '')
            partner_df = pd.read_csv(os.path.join(self.processed_data_dir, partner_file))
            
            # æ·»åŠ åˆä½œæ–¹æ ‡è¯†
            partner_df['partner'] = partner_name
            partner_df['partner_label'] = partner_df['label']
            
            all_data.append(partner_df)
            print(f"  {partner_name}: {len(partner_df)} æ¡è®°å½•")
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        combined_df = pd.concat(all_data, ignore_index=True)
        
        # ç§»é™¤åŸå§‹labelåˆ—
        if 'label' in combined_df.columns:
            combined_df = combined_df.drop('label', axis=1)
        
        # è·å–ç‰¹å¾åˆ—åï¼ˆæ’é™¤partnerå’Œpartner_labelï¼‰
        self.feature_columns = [col for col in combined_df.columns 
                               if col not in ['partner', 'partner_label']]
        
        print(f"åˆå¹¶å®Œæˆï¼Œæ•°æ®å½¢çŠ¶: {combined_df.shape}")
        print(f"ç‰¹å¾åˆ—æ•°: {len(self.feature_columns)}")
        print(f"æ€»æ ·æœ¬æ•°: {len(combined_df)}")
        
        return combined_df
    
    def preprocess_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ç‰¹å¾é¢„å¤„ç†
        
        Args:
            df: åŸå§‹æ•°æ®
            
        Returns:
            å¤„ç†åçš„ç‰¹å¾æ•°æ®
        """
        print("å¼€å§‹ç‰¹å¾é¢„å¤„ç†...")
        processed_df = df.copy()
        
        # 1. å…¬å¸åç§°è¿‡æ»¤è§„åˆ™ç‰¹å¾
        if 'companyInfo.companyName' in processed_df.columns:
            for rule_name, keywords in self.company_filter_rules.items():
                col_name = f'company_filter_{rule_name}'
                processed_df[col_name] = processed_df['companyInfo.companyName'].apply(
                    lambda x: self._check_company_keywords(x, keywords) if pd.notna(x) else 0
                )
            # ç§»é™¤åŸå§‹å…¬å¸åç§°åˆ—
            processed_df = processed_df.drop('companyInfo.companyName', axis=1)
        
        # 2. å­¦å†ç¼–ç  (JUNIOR=1, ..., DOCTOR=6)
        if 'degree' in processed_df.columns:
            degree_mapping = {
                'JUNIOR': 1, 'SENIOR': 2, 'COLLEGE': 3, 
                'BACHELOR': 4, 'MASTER': 5, 'DOCTOR': 6
            }
            processed_df['degree_encoded'] = processed_df['degree'].map(degree_mapping)
            processed_df = processed_df.drop('degree', axis=1)
        
        # 3. æ”¶å…¥ç­‰çº§ç¼–ç  (A=1, B=2, C=3, D=4)
        if 'income' in processed_df.columns:
            income_mapping = {'A': 1, 'B': 2, 'C': 3, 'D': 4}
            processed_df['income_encoded'] = processed_df['income'].map(income_mapping)
            processed_df = processed_df.drop('income', axis=1)
        
        # 4. å¤„ç†åˆ†ç±»ç‰¹å¾
        categorical_features = [
            'bankCardInfo.bankCode', 'city', 'companyInfo.industry', 
            'companyInfo.occupation', 'customerSource', 'idInfo.gender',
            'idInfo.nation', 'jobFunctions', 'linkmanList.0.relationship',
            'linkmanList.1.relationship', 'maritalStatus', 'province',
            'purpose', 'resideFunctions'
        ]
        
        for feature in categorical_features:
            if feature in processed_df.columns:
                # å¤„ç†ç¼ºå¤±å€¼
                processed_df[feature] = processed_df[feature].fillna('UNKNOWN')
                
                # æ ‡ç­¾ç¼–ç 
                if feature not in self.encoders:
                    self.encoders[feature] = LabelEncoder()
                    processed_df[f'{feature}_encoded'] = self.encoders[feature].fit_transform(
                        processed_df[feature].astype(str)
                    )
                else:
                    # å¤„ç†æ–°çš„ç±»åˆ«
                    try:
                        processed_df[f'{feature}_encoded'] = self.encoders[feature].transform(
                            processed_df[feature].astype(str)
                        )
                    except ValueError:
                        # æ–°ç±»åˆ«ç”¨-1è¡¨ç¤º
                        processed_df[f'{feature}_encoded'] = processed_df[feature].apply(
                            lambda x: self.encoders[feature].transform([x])[0] 
                            if x in self.encoders[feature].classes_ else -1
                        )
                
                # ç§»é™¤åŸå§‹åˆ—
                processed_df = processed_df.drop(feature, axis=1)
        
        # 5. å¤„ç†æ•°å€¼ç‰¹å¾çš„ç¼ºå¤±å€¼
        numerical_features = ['amount', 'idInfo.birthDate', 'idInfo.validityDate', 'term']
        for feature in numerical_features:
            if feature in processed_df.columns:
                processed_df[feature] = pd.to_numeric(processed_df[feature], errors='coerce')
                processed_df[feature] = processed_df[feature].fillna(processed_df[feature].median())
        
        print(f"é¢„å¤„ç†å®Œæˆï¼Œç‰¹å¾ç»´åº¦: {processed_df.shape[1]}")
        return processed_df
    
    def _check_company_keywords(self, company_name: str, keywords: List[str]) -> int:
        """
        æ£€æŸ¥å…¬å¸åç§°æ˜¯å¦åŒ…å«å…³é”®è¯
        
        Args:
            company_name: å…¬å¸åç§°
            keywords: å…³é”®è¯åˆ—è¡¨
            
        Returns:
            1 if åŒ…å«å…³é”®è¯, 0 otherwise
        """
        if not company_name:
            return 0
        
        for keyword in keywords:
            if keyword in company_name:
                return 1
        return 0
    
    def prepare_training_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, Dict[str, np.ndarray]]:
        """
        å‡†å¤‡è®­ç»ƒæ•°æ®
        
        Args:
            df: é¢„å¤„ç†åçš„æ•°æ®
            
        Returns:
            ç‰¹å¾çŸ©é˜µXå’Œæ¯ä¸ªåˆä½œæ–¹çš„æ ‡ç­¾å­—å…¸
        """
        # åˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾
        feature_columns = [col for col in df.columns 
                          if col not in ['partner', 'partner_label']]
        
        X = df[feature_columns].values
        
        # ç‰¹å¾æ ‡å‡†åŒ–
        X = self.scaler.fit_transform(X)
        
        # ä¸ºæ¯ä¸ªåˆä½œæ–¹å‡†å¤‡æ ‡ç­¾
        Y_dict = {}
        for partner in self.partners:
            partner_mask = df['partner'] == partner
            partner_indices = df.index[partner_mask].tolist()
            Y_dict[partner] = {
                'X_indices': partner_indices,
                'labels': df.loc[partner_mask, 'partner_label'].values
            }
        
        print(f"è®­ç»ƒæ•°æ®å‡†å¤‡å®Œæˆ:")
        print(f"  ç‰¹å¾ç»´åº¦: {X.shape}")
        print(f"  å„åˆä½œæ–¹æ•°æ®åˆ†å¸ƒ:")
        for partner in self.partners:
            partner_data = Y_dict[partner]
            pass_rate = partner_data['labels'].mean()
            print(f"    {partner}: {len(partner_data['labels'])} æ ·æœ¬, é€šè¿‡ç‡ {pass_rate:.3f}")
        
        return X, Y_dict
    
    def train_models_with_imbalance_handling(self, X: np.ndarray, Y_dict: Dict[str, np.ndarray], 
                                           strategy: str = "class_weight"):
        """
        è®­ç»ƒæ¨¡å‹ - å¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜
        
        Args:
            X: ç‰¹å¾çŸ©é˜µ
            Y_dict: æ¯ä¸ªåˆä½œæ–¹çš„æ ‡ç­¾å­—å…¸
            strategy: å¤„ç†ä¸å¹³è¡¡çš„ç­–ç•¥
                - "class_weight": ä½¿ç”¨ç±»åˆ«æƒé‡
                - "smote": ä½¿ç”¨SMOTEè¿‡é‡‡æ ·
                - "undersampling": ä½¿ç”¨æ¬ é‡‡æ ·
                - "combine": ä½¿ç”¨SMOTE+Tomekç»„åˆæ–¹æ³•
                - "threshold": è°ƒæ•´åˆ†ç±»é˜ˆå€¼
        """
        print(f"å¼€å§‹è®­ç»ƒæ¨¡å‹ - ä½¿ç”¨ {strategy} ç­–ç•¥å¤„ç†ç±»åˆ«ä¸å¹³è¡¡...")
        
        strategies_results = {}
        
        # ä¸ºæ¯ä¸ªåˆä½œæ–¹è®­ç»ƒå•ç‹¬çš„åˆ†ç±»å™¨
        for partner in self.partners:
            print(f"\nè®­ç»ƒ {partner} æ¨¡å‹...")
            
            partner_data = Y_dict[partner]
            X_partner = X[partner_data['X_indices']]
            y_partner = partner_data['labels']
            
            # æ£€æŸ¥æ•°æ®é‡å’Œç±»åˆ«åˆ†å¸ƒ
            if len(y_partner) < 10:
                print(f"  æ•°æ®é‡å¤ªå°‘ ({len(y_partner)} æ ·æœ¬), è·³è¿‡è®­ç»ƒ")
                continue
            
            pos_count = np.sum(y_partner)
            neg_count = len(y_partner) - pos_count
            pos_rate = pos_count / len(y_partner)
            
            print(f"  æ•°æ®åˆ†å¸ƒ: æ­£ç±» {pos_count}, è´Ÿç±» {neg_count}, æ­£ç±»ç‡ {pos_rate:.3f}")
            
            if pos_count < 5:
                print(f"  æ­£ç±»æ ·æœ¬å¤ªå°‘ ({pos_count} ä¸ª), è·³è¿‡è®­ç»ƒ")
                continue
            
            # æ ¹æ®ç­–ç•¥é€‰æ‹©ä¸åŒçš„å¤„ç†æ–¹æ³•
            if strategy == "class_weight":
                model, cv_results = self._train_with_class_weight(X_partner, y_partner, partner)
            elif strategy == "smote":
                model, cv_results = self._train_with_smote(X_partner, y_partner, partner)
            elif strategy == "undersampling":
                model, cv_results = self._train_with_undersampling(X_partner, y_partner, partner)
            elif strategy == "combine":
                model, cv_results = self._train_with_combine_sampling(X_partner, y_partner, partner)
            elif strategy == "threshold":
                model, cv_results = self._train_with_threshold_tuning(X_partner, y_partner, partner)
            else:
                model, cv_results = self._train_baseline(X_partner, y_partner, partner)
            
            if model is not None:
                self.models[partner] = model
                strategies_results[partner] = cv_results
        
        return strategies_results
    
    def _train_with_class_weight(self, X: np.ndarray, y: np.ndarray, partner: str):
        """ä½¿ç”¨ç±»åˆ«æƒé‡å¤„ç†ä¸å¹³è¡¡"""
        print(f"  ç­–ç•¥: ç±»åˆ«æƒé‡å¹³è¡¡")
        
        # è®¡ç®—ç±»åˆ«æƒé‡
        class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)
        class_weight_dict = {0: class_weights[0], 1: class_weights[1]}
        
        print(f"  ç±»åˆ«æƒé‡: {class_weight_dict}")
        
        model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            min_samples_split=min(5, len(y) // 10),
            min_samples_leaf=2,
            class_weight=class_weight_dict,  # å…³é”®ï¼šä½¿ç”¨ç±»åˆ«æƒé‡
            random_state=42,
            n_jobs=-1
        )
        
        # äº¤å‰éªŒè¯
        cv_results = self._evaluate_model(model, X, y)
        
        # åœ¨å…¨é‡æ•°æ®ä¸Šè®­ç»ƒæœ€ç»ˆæ¨¡å‹
        model.fit(X, y)
        
        return model, cv_results
    
    def _train_with_smote(self, X: np.ndarray, y: np.ndarray, partner: str):
        """ä½¿ç”¨SMOTEè¿‡é‡‡æ ·å¤„ç†ä¸å¹³è¡¡"""
        print(f"  ç­–ç•¥: SMOTEè¿‡é‡‡æ ·")
        
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„å°‘æ•°ç±»æ ·æœ¬è¿›è¡ŒSMOTE
            min_samples = min(np.bincount(y.astype(int)))
            if min_samples < 2:
                print(f"  å°‘æ•°ç±»æ ·æœ¬å¤ªå°‘ï¼Œå›é€€åˆ°ç±»åˆ«æƒé‡æ–¹æ³•")
                return self._train_with_class_weight(X, y, partner)
            
            # ä½¿ç”¨SMOTEè¿›è¡Œè¿‡é‡‡æ ·
            smote = SMOTE(random_state=42, k_neighbors=min(5, min_samples-1))
            
            # åˆ†å±‚äº¤å‰éªŒè¯ï¼Œæ¯æŠ˜å†…éƒ¨ä½¿ç”¨SMOTE
            skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
            cv_scores = {'auc': [], 'accuracy': [], 'recall': [], 'f1': []}
            
            for train_idx, val_idx in skf.split(X, y):
                X_train_fold, X_val_fold = X[train_idx], X[val_idx]
                y_train_fold, y_val_fold = y[train_idx], y[val_idx]
                
                # åœ¨è®­ç»ƒé›†ä¸Šåº”ç”¨SMOTE
                X_train_resampled, y_train_resampled = smote.fit_resample(X_train_fold, y_train_fold)
                
                # è®­ç»ƒæ¨¡å‹
                model_fold = RandomForestClassifier(
                    n_estimators=100,
                    max_depth=10,
                    min_samples_split=5,
                    min_samples_leaf=2,
                    random_state=42,
                    n_jobs=-1
                )
                model_fold.fit(X_train_resampled, y_train_resampled)
                
                # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
                y_pred_proba = model_fold.predict_proba(X_val_fold)[:, 1]
                y_pred = (y_pred_proba > 0.5).astype(int)
                
                from sklearn.metrics import roc_auc_score, accuracy_score, recall_score, f1_score
                cv_scores['auc'].append(roc_auc_score(y_val_fold, y_pred_proba))
                cv_scores['accuracy'].append(accuracy_score(y_val_fold, y_pred))
                cv_scores['recall'].append(recall_score(y_val_fold, y_pred))
                cv_scores['f1'].append(f1_score(y_val_fold, y_pred))
            
            # è®¡ç®—å¹³å‡æ€§èƒ½
            cv_results = {f'test_{k}': np.array(v) for k, v in cv_scores.items()}
            
            # åœ¨å…¨é‡æ•°æ®ä¸Šåº”ç”¨SMOTEå¹¶è®­ç»ƒæœ€ç»ˆæ¨¡å‹
            X_resampled, y_resampled = smote.fit_resample(X, y)
            print(f"  SMOTEå: åŸå§‹ {len(y)} -> å¹³è¡¡ {len(y_resampled)} æ ·æœ¬")
            
            model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                n_jobs=-1
            )
            model.fit(X_resampled, y_resampled)
            
            return model, cv_results
            
        except Exception as e:
            print(f"  SMOTEå¤±è´¥: {e}, å›é€€åˆ°ç±»åˆ«æƒé‡æ–¹æ³•")
            return self._train_with_class_weight(X, y, partner)
    
    def _train_with_undersampling(self, X: np.ndarray, y: np.ndarray, partner: str):
        """ä½¿ç”¨æ¬ é‡‡æ ·å¤„ç†ä¸å¹³è¡¡"""
        print(f"  ç­–ç•¥: éšæœºæ¬ é‡‡æ ·")
        
        # æ£€æŸ¥å¤šæ•°ç±»æ˜¯å¦æœ‰è¶³å¤Ÿæ ·æœ¬è¿›è¡Œæ¬ é‡‡æ ·
        pos_count = np.sum(y)
        neg_count = len(y) - pos_count
        
        if neg_count < pos_count * 2:
            print(f"  è´Ÿç±»æ ·æœ¬ä¸è¶³ä»¥è¿›è¡Œæ¬ é‡‡æ ·ï¼Œå›é€€åˆ°ç±»åˆ«æƒé‡æ–¹æ³•")
            return self._train_with_class_weight(X, y, partner)
        
        try:
            # è®¾ç½®é‡‡æ ·ç­–ç•¥ï¼šè´Ÿç±»é‡‡æ ·åˆ°æ­£ç±»çš„2å€
            sampling_strategy = {0: min(pos_count * 2, neg_count), 1: pos_count}
            
            undersampler = RandomUnderSampler(
                sampling_strategy=sampling_strategy,
                random_state=42
            )
            
            # åˆ†å±‚äº¤å‰éªŒè¯
            skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
            cv_scores = {'auc': [], 'accuracy': [], 'recall': [], 'f1': []}
            
            for train_idx, val_idx in skf.split(X, y):
                X_train_fold, X_val_fold = X[train_idx], X[val_idx]
                y_train_fold, y_val_fold = y[train_idx], y[val_idx]
                
                # åœ¨è®­ç»ƒé›†ä¸Šåº”ç”¨æ¬ é‡‡æ ·
                X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train_fold, y_train_fold)
                
                # è®­ç»ƒæ¨¡å‹
                model_fold = RandomForestClassifier(
                    n_estimators=100,
                    max_depth=10,
                    min_samples_split=5,
                    min_samples_leaf=2,
                    random_state=42,
                    n_jobs=-1
                )
                model_fold.fit(X_train_resampled, y_train_resampled)
                
                # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
                y_pred_proba = model_fold.predict_proba(X_val_fold)[:, 1]
                y_pred = (y_pred_proba > 0.5).astype(int)
                
                from sklearn.metrics import roc_auc_score, accuracy_score, recall_score, f1_score
                cv_scores['auc'].append(roc_auc_score(y_val_fold, y_pred_proba))
                cv_scores['accuracy'].append(accuracy_score(y_val_fold, y_pred))
                cv_scores['recall'].append(recall_score(y_val_fold, y_pred))
                cv_scores['f1'].append(f1_score(y_val_fold, y_pred))
            
            cv_results = {f'test_{k}': np.array(v) for k, v in cv_scores.items()}
            
            # åœ¨å…¨é‡æ•°æ®ä¸Šåº”ç”¨æ¬ é‡‡æ ·å¹¶è®­ç»ƒæœ€ç»ˆæ¨¡å‹
            X_resampled, y_resampled = undersampler.fit_resample(X, y)
            print(f"  æ¬ é‡‡æ ·å: åŸå§‹ {len(y)} -> å¹³è¡¡ {len(y_resampled)} æ ·æœ¬")
            
            model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                n_jobs=-1
            )
            model.fit(X_resampled, y_resampled)
            
            return model, cv_results
            
        except Exception as e:
            print(f"  æ¬ é‡‡æ ·å¤±è´¥: {e}, å›é€€åˆ°ç±»åˆ«æƒé‡æ–¹æ³•")
            return self._train_with_class_weight(X, y, partner)
    
    def _train_with_combine_sampling(self, X: np.ndarray, y: np.ndarray, partner: str):
        """ä½¿ç”¨SMOTE+Tomekç»„åˆé‡‡æ ·å¤„ç†ä¸å¹³è¡¡"""
        print(f"  ç­–ç•¥: SMOTE+Tomekç»„åˆé‡‡æ ·")
        
        try:
            min_samples = min(np.bincount(y.astype(int)))
            if min_samples < 2:
                return self._train_with_class_weight(X, y, partner)
            
            # ä½¿ç”¨SMOTETomekç»„åˆæ–¹æ³•
            smote_tomek = SMOTETomek(
                smote=SMOTE(random_state=42, k_neighbors=min(5, min_samples-1)),
                random_state=42
            )
            
            # äº¤å‰éªŒè¯
            skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
            cv_scores = {'auc': [], 'accuracy': [], 'recall': [], 'f1': []}
            
            for train_idx, val_idx in skf.split(X, y):
                X_train_fold, X_val_fold = X[train_idx], X[val_idx]
                y_train_fold, y_val_fold = y[train_idx], y[val_idx]
                
                X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train_fold, y_train_fold)
                
                model_fold = RandomForestClassifier(
                    n_estimators=100,
                    max_depth=10,
                    min_samples_split=5,
                    min_samples_leaf=2,
                    random_state=42,
                    n_jobs=-1
                )
                model_fold.fit(X_train_resampled, y_train_resampled)
                
                y_pred_proba = model_fold.predict_proba(X_val_fold)[:, 1]
                y_pred = (y_pred_proba > 0.5).astype(int)
                
                from sklearn.metrics import roc_auc_score, accuracy_score, recall_score, f1_score
                cv_scores['auc'].append(roc_auc_score(y_val_fold, y_pred_proba))
                cv_scores['accuracy'].append(accuracy_score(y_val_fold, y_pred))
                cv_scores['recall'].append(recall_score(y_val_fold, y_pred))
                cv_scores['f1'].append(f1_score(y_val_fold, y_pred))
            
            cv_results = {f'test_{k}': np.array(v) for k, v in cv_scores.items()}
            
            # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
            X_resampled, y_resampled = smote_tomek.fit_resample(X, y)
            print(f"  ç»„åˆé‡‡æ ·å: åŸå§‹ {len(y)} -> å¤„ç†å {len(y_resampled)} æ ·æœ¬")
            
            model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                n_jobs=-1
            )
            model.fit(X_resampled, y_resampled)
            
            return model, cv_results
            
        except Exception as e:
            print(f"  ç»„åˆé‡‡æ ·å¤±è´¥: {e}, å›é€€åˆ°ç±»åˆ«æƒé‡æ–¹æ³•")
            return self._train_with_class_weight(X, y, partner)
    
    def _train_with_threshold_tuning(self, X: np.ndarray, y: np.ndarray, partner: str):
        """é€šè¿‡è°ƒæ•´åˆ†ç±»é˜ˆå€¼å¤„ç†ä¸å¹³è¡¡"""
        print(f"  ç­–ç•¥: åˆ†ç±»é˜ˆå€¼è°ƒä¼˜")
        
        model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            min_samples_split=min(5, len(y) // 10),
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )
        
        # äº¤å‰éªŒè¯å¯»æ‰¾æœ€ä¼˜é˜ˆå€¼
        skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
        best_threshold = 0.5
        best_f1 = 0
        
        cv_scores = {'auc': [], 'accuracy': [], 'recall': [], 'f1': []}
        
        for train_idx, val_idx in skf.split(X, y):
            X_train_fold, X_val_fold = X[train_idx], X[val_idx]
            y_train_fold, y_val_fold = y[train_idx], y[val_idx]
            
            model_fold = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                min_samples_split=5,
                min_samples_leaf=2,
                random_state=42,
                n_jobs=-1
            )
            model_fold.fit(X_train_fold, y_train_fold)
            
            # è·å–é¢„æµ‹æ¦‚ç‡
            y_pred_proba = model_fold.predict_proba(X_val_fold)[:, 1]
            
            # å¯»æ‰¾æœ€ä¼˜é˜ˆå€¼
            thresholds = np.arange(0.1, 0.9, 0.05)
            fold_best_f1 = 0
            fold_best_threshold = 0.5
            
            for threshold in thresholds:
                y_pred_thresh = (y_pred_proba >= threshold).astype(int)
                from sklearn.metrics import f1_score
                f1 = f1_score(y_val_fold, y_pred_thresh)
                if f1 > fold_best_f1:
                    fold_best_f1 = f1
                    fold_best_threshold = threshold
            
            # ä½¿ç”¨æœ€ä¼˜é˜ˆå€¼è¿›è¡Œé¢„æµ‹
            y_pred = (y_pred_proba >= fold_best_threshold).astype(int)
            
            from sklearn.metrics import roc_auc_score, accuracy_score, recall_score, f1_score
            cv_scores['auc'].append(roc_auc_score(y_val_fold, y_pred_proba))
            cv_scores['accuracy'].append(accuracy_score(y_val_fold, y_pred))
            cv_scores['recall'].append(recall_score(y_val_fold, y_pred))
            cv_scores['f1'].append(f1_score(y_val_fold, y_pred))
            
            if fold_best_f1 > best_f1:
                best_f1 = fold_best_f1
                best_threshold = fold_best_threshold
        
        print(f"  æœ€ä¼˜é˜ˆå€¼: {best_threshold:.3f}")
        
        cv_results = {f'test_{k}': np.array(v) for k, v in cv_scores.items()}
        
        # è®­ç»ƒæœ€ç»ˆæ¨¡å‹
        model.fit(X, y)
        
        # ä¿å­˜æœ€ä¼˜é˜ˆå€¼ä¾›é¢„æµ‹ä½¿ç”¨
        setattr(model, 'optimal_threshold', best_threshold)
        
        return model, cv_results
    
    def _evaluate_model(self, model, X: np.ndarray, y: np.ndarray):
        """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
        scoring = {
            'roc_auc': 'roc_auc',
            'accuracy': 'accuracy', 
            'recall': 'recall',
            'f1': 'f1',
        }
        
        cv_results = cross_validate(
            model, X, y, scoring=scoring, 
            cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42)
        )
        
        return cv_results
    
    def _print_results(self, partner: str, cv_results: dict):
        """æ‰“å°ç»“æœ"""
        print(f"  {partner} äº¤å‰éªŒè¯ç»“æœ:")
        auc_key = 'test_roc_auc' if 'test_roc_auc' in cv_results else 'test_auc'
        print(f"    AUC: {cv_results[auc_key].mean():.3f} (Â±{cv_results[auc_key].std():.3f})")
        print(f"    å‡†ç¡®ç‡: {cv_results['test_accuracy'].mean():.3f} (Â±{cv_results['test_accuracy'].std():.3f})")
        print(f"    å¬å›ç‡: {cv_results['test_recall'].mean():.3f} (Â±{cv_results['test_recall'].std():.3f})")
        print(f"    F1åˆ†æ•°: {cv_results['test_f1'].mean():.3f} (Â±{cv_results['test_f1'].std():.3f})")
    
    def compare_imbalance_strategies(self, X: np.ndarray, Y_dict: Dict[str, np.ndarray]):
        """
        æ¯”è¾ƒä¸åŒçš„ä¸å¹³è¡¡å¤„ç†ç­–ç•¥
        
        Args:
            X: ç‰¹å¾çŸ©é˜µ
            Y_dict: æ¯ä¸ªåˆä½œæ–¹çš„æ ‡ç­¾å­—å…¸
        """
        print("=== æ¯”è¾ƒä¸åŒçš„ç±»åˆ«ä¸å¹³è¡¡å¤„ç†ç­–ç•¥ ===\n")
        
        strategies = [
            "baseline",
            "class_weight", 
            "smote",
            "undersampling",
            "combine",
            "threshold"
        ]
        
        all_results = {}
        
        for strategy in strategies:
            print(f"\n{'='*50}")
            print(f"ç­–ç•¥: {strategy.upper()}")
            print(f"{'='*50}")
            
            if strategy == "baseline":
                results = self._train_baseline_all(X, Y_dict)
            else:
                results = self.train_models_with_imbalance_handling(X, Y_dict, strategy)
            
            all_results[strategy] = results
        
        # æ€»ç»“æ¯”è¾ƒç»“æœ
        self._summarize_strategy_comparison(all_results)
        
        return all_results
    
    def _train_baseline_all(self, X: np.ndarray, Y_dict: Dict[str, np.ndarray]):
        """è®­ç»ƒåŸºçº¿æ¨¡å‹ï¼ˆåŸå§‹æ–¹æ³•ï¼‰"""
        results = {}
        
        for partner in self.partners:
            partner_data = Y_dict[partner]
            X_partner = X[partner_data['X_indices']]
            y_partner = partner_data['labels']
            
            if len(y_partner) < 10 or np.sum(y_partner) < 5:
                continue
            
            model, cv_results = self._train_baseline(X_partner, y_partner, partner)
            if model is not None:
                results[partner] = cv_results
        
        return results
    
    def _train_baseline(self, X: np.ndarray, y: np.ndarray, partner: str):
        """åŸºçº¿æ¨¡å‹è®­ç»ƒ"""
        print(f"  ç­–ç•¥: åŸºçº¿æ¨¡å‹ï¼ˆæ— ç‰¹æ®Šå¤„ç†ï¼‰")
        
        model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            min_samples_split=min(5, len(y) // 10),
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )
        
        cv_results = self._evaluate_model(model, X, y)
        self._print_results(partner, cv_results)
        
        model.fit(X, y)
        return model, cv_results
    
    def _summarize_strategy_comparison(self, all_results: Dict[str, Dict]):
        """æ€»ç»“ç­–ç•¥æ¯”è¾ƒç»“æœ"""
        print(f"\n{'='*80}")
        print("ç­–ç•¥æ¯”è¾ƒæ€»ç»“")
        print(f"{'='*80}")
        
        # ä¸ºæ¯ä¸ªåˆä½œæ–¹æ‰¾åˆ°æœ€ä½³ç­–ç•¥
        for partner in self.partners:
            partner_results = {}
            
            for strategy, results in all_results.items():
                if partner in results:
                    cv_result = results[partner]
                    # ä½¿ç”¨F1åˆ†æ•°ä½œä¸ºä¸»è¦è¯„ä¼°æŒ‡æ ‡
                    avg_f1 = cv_result['test_f1'].mean() if 'test_f1' in cv_result else 0
                    avg_auc = cv_result['test_auc'].mean() if 'test_auc' in cv_result else cv_result['test_roc_auc'].mean()
                    avg_recall = cv_result['test_recall'].mean()
                    
                    partner_results[strategy] = {
                        'f1': avg_f1,
                        'auc': avg_auc, 
                        'recall': avg_recall
                    }
            
            if partner_results:
                print(f"\n{partner}:")
                # æŒ‰F1åˆ†æ•°æ’åº
                sorted_strategies = sorted(partner_results.items(), 
                                         key=lambda x: x[1]['f1'], reverse=True)
                
                for i, (strategy, metrics) in enumerate(sorted_strategies):
                    status = "ğŸ† æœ€ä½³" if i == 0 else f"  #{i+1}"
                    print(f"  {status} {strategy:15} F1: {metrics['f1']:.3f}  AUC: {metrics['auc']:.3f}  å¬å›ç‡: {metrics['recall']:.3f}")
    
    def train_models(self, X: np.ndarray, Y_dict: Dict[str, np.ndarray]):
        """
        ä¿æŒåŸæœ‰æ¥å£ï¼Œé»˜è®¤ä½¿ç”¨ç±»åˆ«æƒé‡ç­–ç•¥
        """
        return self.train_models_with_imbalance_handling(X, Y_dict, strategy="class_weight")
        
    
    def predict_partner_probabilities(self, X: np.ndarray) -> Dict[str, np.ndarray]:
        """
        é¢„æµ‹æ¯ä¸ªåˆä½œæ–¹çš„é€šè¿‡æ¦‚ç‡
        
        Args:
            X: ç‰¹å¾çŸ©é˜µ
            
        Returns:
            æ¯ä¸ªåˆä½œæ–¹çš„é€šè¿‡æ¦‚ç‡
        """
        probabilities = {}
        
        for partner in self.partners:
            if partner in self.models:
                # è·å–æ­£ç±»æ¦‚ç‡
                proba = self.models[partner].predict_proba(X)[:, 1]
                probabilities[partner] = proba
            
        return probabilities
    
    def recommend_partners(self, user_features: np.ndarray, k: int = 3, 
                          min_probability: float = 0.3) -> List[Tuple[str, float]]:
        """
        ä¸ºç”¨æˆ·æ¨èåˆä½œæ–¹
        
        Args:
            user_features: ç”¨æˆ·ç‰¹å¾ (1, n_features)
            k: æ¨èçš„åˆä½œæ–¹æ•°é‡ä¸Šé™
            min_probability: æœ€å°é€šè¿‡æ¦‚ç‡é˜ˆå€¼
            
        Returns:
            æ¨èçš„åˆä½œæ–¹åˆ—è¡¨ [(partner_name, probability), ...]
        """
        # é¢„æµ‹æ‰€æœ‰åˆä½œæ–¹çš„é€šè¿‡æ¦‚ç‡
        probabilities = self.predict_partner_probabilities(user_features)
        
        # è¿‡æ»¤ä½æ¦‚ç‡åˆä½œæ–¹
        filtered_partners = [
            (partner, prob[0]) for partner, prob in probabilities.items() 
            if prob[0] >= min_probability
        ]
        
        # æŒ‰æ¦‚ç‡æ’åºå¹¶å–å‰kä¸ª
        filtered_partners.sort(key=lambda x: x[1], reverse=True)
        
        return filtered_partners[:k]
    
    def save_model(self, model_path: str = "loan_distribution_model"):
        """
        ä¿å­˜æ¨¡å‹
        
        Args:
            model_path: æ¨¡å‹ä¿å­˜è·¯å¾„
        """
        os.makedirs(model_path, exist_ok=True)
        
        # ä¿å­˜å„ä¸ªç»„ä»¶
        joblib.dump(self.models, os.path.join(model_path, 'models.pkl'))
        joblib.dump(self.encoders, os.path.join(model_path, 'encoders.pkl'))
        joblib.dump(self.scaler, os.path.join(model_path, 'scaler.pkl'))
        joblib.dump(self.partners, os.path.join(model_path, 'partners.pkl'))
        
        print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
    
    def load_model(self, model_path: str = "loan_distribution_model"):
        """
        åŠ è½½æ¨¡å‹
        
        Args:
            model_path: æ¨¡å‹åŠ è½½è·¯å¾„
        """
        self.models = joblib.load(os.path.join(model_path, 'models.pkl'))
        self.encoders = joblib.load(os.path.join(model_path, 'encoders.pkl'))
        self.scaler = joblib.load(os.path.join(model_path, 'scaler.pkl'))
        self.partners = joblib.load(os.path.join(model_path, 'partners.pkl'))
        
        print(f"æ¨¡å‹å·²ä» {model_path} åŠ è½½")

def main():
    """
    ä¸»å‡½æ•°ï¼šæ¼”ç¤ºå®Œæ•´çš„è®­ç»ƒå’Œé¢„æµ‹æµç¨‹
    """
    print("=== è´·æ¬¾åˆ†å‘æ™ºèƒ½å†³ç­–æ¨¡å‹ - å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ ===")
    
    # 1. åˆå§‹åŒ–æ¨¡å‹
    model = LoanDistributionModel()
    
    # 2. åŠ è½½å’Œé¢„å¤„ç†æ•°æ®
    combined_data = model.load_and_combine_data()
    processed_data = model.preprocess_features(combined_data)
    
    # 3. å‡†å¤‡è®­ç»ƒæ•°æ®
    X, Y_dict = model.prepare_training_data(processed_data)
    
    # 4. æ¯”è¾ƒä¸åŒçš„ä¸å¹³è¡¡å¤„ç†ç­–ç•¥
    comparison_results = model.compare_imbalance_strategies(X, Y_dict)
    
    print(f"\n{'='*80}")
    print("æ¨èç­–ç•¥æ€»ç»“:")
    print(f"{'='*80}")
    print("åŸºäºä»¥ä¸Šæ¯”è¾ƒç»“æœï¼Œæ¨èä½¿ç”¨ä»¥ä¸‹ç­–ç•¥ï¼š")
    print("1. å¯¹äºé€šè¿‡ç‡æä½çš„åˆä½œæ–¹ï¼ˆ<2%ï¼‰: class_weight æˆ– threshold ç­–ç•¥")
    print("2. å¯¹äºé€šè¿‡ç‡è¾ƒä½çš„åˆä½œæ–¹ï¼ˆ2-10%ï¼‰: smote æˆ– combine ç­–ç•¥") 
    print("3. å¯¹äºé€šè¿‡ç‡ä¸­ç­‰çš„åˆä½œæ–¹ï¼ˆ10-30%ï¼‰: class_weight ç­–ç•¥")
    print("4. å»ºè®®ä¼˜å…ˆå…³æ³¨å¬å›ç‡å’ŒF1åˆ†æ•°ï¼Œè€Œéå‡†ç¡®ç‡")
    print("5. å¯ä»¥æ ¹æ®ä¸šåŠ¡éœ€æ±‚è°ƒæ•´è¯„ä¼°æŒ‡æ ‡çš„æƒé‡")

def demo_single_strategy():
    """
    æ¼”ç¤ºå•ä¸€ç­–ç•¥çš„è¯¦ç»†æ•ˆæœ
    """
    print("=== å•ä¸€ç­–ç•¥æ¼”ç¤ºï¼šç±»åˆ«æƒé‡æ–¹æ³• ===")
    
    model = LoanDistributionModel()
    combined_data = model.load_and_combine_data()
    processed_data = model.preprocess_features(combined_data)
    X, Y_dict = model.prepare_training_data(processed_data)

    # ä½¿ç”¨SMOTEç­–ç•¥è®­ç»ƒ
    results = model.train_models_with_imbalance_handling(X, Y_dict, strategy="smote")
    
    # æ‰“å°è¯¦ç»†ç»“æœ
    for partner, cv_result in results.items():
        print(f"\n{partner} è¯¦ç»†ç»“æœ:")
        auc_key = 'test_roc_auc' if 'test_roc_auc' in cv_result else 'test_auc'
        print(f"  AUC: {cv_result[auc_key].mean():.3f} Â± {cv_result[auc_key].std():.3f}")
        print(f"  å‡†ç¡®ç‡: {cv_result['test_accuracy'].mean():.3f} Â± {cv_result['test_accuracy'].std():.3f}")
        print(f"  å¬å›ç‡: {cv_result['test_recall'].mean():.3f} Â± {cv_result['test_recall'].std():.3f}")
        print(f"  F1åˆ†æ•°: {cv_result['test_f1'].mean():.3f} Â± {cv_result['test_f1'].std():.3f}")
    
    return model

if __name__ == "__main__":
    main()
